{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9b385-0a90-404d-8405-3976446c171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crack 2.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9ce38-36fc-4d63-a5d8-27ccfe145a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as L\n",
    "\n",
    "torch.manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa136c1b-6a79-49bc-9369-184fbcaa6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_csv(\"../names.txt\",names = [\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bfe95-e0a7-4f00-9476-42648ee3f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stoi = {}\n",
    "all_itos ={}\n",
    "\n",
    "all = set(\"\".join(names.label+\".\"))\n",
    "for index,(chr) in enumerate(sorted(all)):\n",
    "    all_stoi[chr] = index\n",
    "\n",
    "for index,(chr) in enumerate(sorted(all)):\n",
    "    all_itos[index] = chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e636a-571c-4dd2-8797-9c342677da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "blocks = 3\n",
    "\n",
    "for word in names.label:\n",
    "    wx = [(\".\"* (blocks - i)) + word  for i in range(blocks)]\n",
    "    wy = [y.append(all_stoi[i]) for i in (word+\".\")]\n",
    "    for ch1,ch2,ch3 in zip(*wx[:len(wx)]):\n",
    "        x.append((all_stoi[ch1],all_stoi[ch2],all_stoi[ch3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072893e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa948882-355c-44c1-b0f4-0659f598594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 70%\n",
    "# val: 10%\n",
    "# test: 30%\n",
    "\n",
    "train_x,val_x,train_y,val_y = train_test_split(x,y,test_size=0.2, shuffle=True,stratify = y, random_state=42)\n",
    "val_x,test_x,val_y,test_y = train_test_split(val_x,val_y,test_size=0.1, shuffle=True,stratify = val_y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_x = torch.tensor(train_x,device = device)\n",
    "train_y = torch.tensor(train_y,device = device)\n",
    "\n",
    "val_x = torch.tensor(val_x,device = device)\n",
    "val_y = torch.tensor(val_y,device = device)\n",
    "\n",
    "test_x = torch.tensor(test_x,device = device)\n",
    "test_y = torch.tensor(test_y,device = device)\n",
    "\n",
    "# print(f\"x: {x.shape} y: {y.shape}\")\n",
    "print(f\"train_x: {train_x.shape} train_y: {train_y.shape}\")\n",
    "print(f\"val_x: {val_x.shape} val_y: {val_y.shape}\")\n",
    "print(f\"test_x: {test_x.shape} test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc970609-656e-4fb8-8eb3-f0dbecda65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data.to(device)\n",
    "        self.labels = labels.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        x =  self.data[idx]\n",
    "        y =  self.labels[idx]\n",
    "        return x,y\n",
    "\n",
    "        \n",
    "train_ds = CustomDataset(train_x,train_y)\n",
    "val_ds = CustomDataset(val_x,val_y)\n",
    "test_ds = CustomDataset(test_x,test_y)\n",
    "\n",
    "train_loader = DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_ds,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_ds,batch_size=32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f88044-f112-4c04-baf7-33bcc3cce05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "c = torch.randn(27,2,device =device)\n",
    "\n",
    "w = torch.randn(6,100,device =device)\n",
    "b = torch.randn(100,device =device)\n",
    "\n",
    "\n",
    "w1 = torch.randn(100,27,device =device)\n",
    "b1 = torch.randn(27,device =device)\n",
    "\n",
    "lr = 0.01 #loss=tensor(2.0388, grad_fn=<NllLossBackward0>)\n",
    "# lr = 0.1\n",
    "\n",
    "parameter = [c,w,b,w1,b1]\n",
    "\n",
    "for p in parameter:\n",
    "    p.requires_grad =True\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(parameter,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72247825-0b0d-46fc-9170-ee182cf2a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossi = []\n",
    "# epochi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0088e36-ca83-456c-b3bc-60bcc3dd1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for epoch in range(10000):\n",
    "    # Forward Pass\n",
    "    # for index,(feature,label) in enumerate(train_loader):\n",
    "        emb = c[train_x] # [32, 3, 2]\n",
    "        z = torch.tanh(emb.view(-1,w.shape[0]) @ w + b)\n",
    "        logits = z @ w1 + b1\n",
    "        loss = F.cross_entropy(logits,train_y)\n",
    "        # lossi.append(loss.item())\n",
    "        # epochi.append(epoch)\n",
    "        \n",
    "        ##Backward Pass\n",
    "        # for p in parameter:\n",
    "        #     p.grad = None\n",
    "        # loss.backward()\n",
    "        # p.data += -lr*p.grad\n",
    "        ## same as\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "end = time.time()\n",
    "delay = end - start\n",
    "print(f\"time: {delay*60}min: {loss.item()=}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905e123-e660-4af0-b28e-c66b14ed20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lossi,epochi)\n",
    "\n",
    "plt.grid(\"minor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70545c4-0a72-4e53-82b8-e6669f99d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16.4423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b06df-22cb-4db7-b2dd-b5ad55ac3f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss\n",
    "for index,(feature,label) in enumerate(train_loader):\n",
    "    emb = c[feature] # [32, 3, 2]\n",
    "    z = torch.tanh(emb.view(-1,w.shape[0]) @ w + b)\n",
    "    logits = z @ w1 + b1\n",
    "    loss = F.cross_entropy(logits,label)\n",
    "\n",
    "print(f\"{loss.item()=}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e385c5-859d-40dc-82e7-a8f0fd749632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val loss\n",
    "for index,(feature,label) in enumerate(val_loader):\n",
    "    emb = c[feature] # [32, 3, 2]\n",
    "    z = torch.tanh(emb.view(-1,w.shape[0]) @ w + b)\n",
    "    logits = z @ w1 + b1\n",
    "    loss = F.cross_entropy(logits,label)\n",
    "\n",
    "print(f\"{loss.item()=}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac10b39",
   "metadata": {},
   "source": [
    "# Automate training with Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f6278ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model\n",
    "class MLnames(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Initialize parameters\n",
    "    self.c = nn.Parameter(torch.randn(27,2,device =device))\n",
    "    self.w = nn.Parameter(torch.randn(6,100,device =device))\n",
    "    self.b = nn.Parameter(torch.randn(100,device =device))\n",
    "    \n",
    "    self.w1 = nn.Parameter(torch.randn(100,27,device =device))\n",
    "    self.b1 = nn.Parameter(torch.randn(27,device =device))\n",
    "\n",
    "    # self.all_layer = nn.Sequential(\n",
    "    #   # Input Layer\n",
    "    #     nn.Linear(6,100),\n",
    "    #     nn.Tanh(),\n",
    "\n",
    "    #   # Hidden Layer\n",
    "    #     nn.Linear(100,100),\n",
    "    #     nn.Tanh(),\n",
    "\n",
    "    #   # Output Layer\n",
    "    #     nn.Linear(100,27)\n",
    "    # )\n",
    "    \n",
    "  # Forward Pass\n",
    "  def forward(self,x):\n",
    "    emb = self.c[x] # [32, 3, 2]\n",
    "    z = torch.tanh(emb.view(-1,self.w.shape[0]) @ w + b)\n",
    "    logits = self.z @ self.w1 + self.b1\n",
    "    return logits\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd946c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the model with PyTorch Lightning\n",
    "class LightningModelExe(L.LightningModule):\n",
    "    def __init__(self,model,lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        output = self.forward(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db5637d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: 182516 train_y: 182516\n",
      "val_x: 41067 val_y: 41067\n",
      "test_x: 4563 test_y: 4563\n"
     ]
    }
   ],
   "source": [
    "xo = []\n",
    "yo = []\n",
    "blocks = 3\n",
    "for word in names.label:\n",
    "    wx = [(\".\"* (blocks - i)) + word  for i in range(blocks)]\n",
    "    wy = [yo.append(all_stoi[i]) for i in (word+\".\")]\n",
    "                       \n",
    "    for ch1,ch2,ch3 in zip(*wx[:len(wx)]):\n",
    "        xo.append((all_stoi[ch1],all_stoi[ch2],all_stoi[ch3]))\n",
    " \n",
    "train_x,val_x,train_y,val_y = train_test_split(xo,yo,test_size=0.2,stratify = y,random_state=42)\n",
    "val_x,test_x,val_y,test_y = train_test_split(val_x,val_y,test_size=0.1,stratify = val_y, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_x = torch.tensor(train_x,device = device)\n",
    "train_y = torch.tensor(train_y,device = device)\n",
    "\n",
    "val_x = torch.tensor(val_x,device = device)\n",
    "val_y = torch.tensor(val_y,device = device)\n",
    "\n",
    "test_x = torch.tensor(test_x,device = device)\n",
    "test_y = torch.tensor(test_y,device = device)\n",
    "\n",
    "\n",
    "train_ = {\"train_x\": train_x, \"train_y\": train_y}\n",
    "valid_ = {\"val_x\": val_x, \"val_y\": val_y}\n",
    "test_ = {\"test_x\": test_x, \"test_y\": test_y}\n",
    "\n",
    "\n",
    "\n",
    "print(f\"train_x: {len(train_x)} train_y: {len(train_y)}\")\n",
    "print(f\"val_x: {len(val_x)} val_y: {len(val_y)}\")\n",
    "print(f\"test_x: {len(test_x)} test_y: {len(test_y)}\")\n",
    "\n",
    "# print(len(xo),len(yo))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x =  self.data[idx]\n",
    "        y =  self.labels[idx]\n",
    "        return x,y\n",
    "        \n",
    "# # Execute the Data Loading with PyTorch Lightning\n",
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"../names.txt\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = pd.read_csv(data_dir,names = [\"label\"])\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str = None,train = train_,valid = valid_,test = test_):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_ds = CustomDataset(train[\"train_x\"], train[\"train_y\"])\n",
    "            self.val_ds = CustomDataset(valid[\"val_x\"], valid[\"val_y\"])\n",
    "        if stage == \"validate\" or stage is None:\n",
    "            self.val_ds = CustomDataset(valid[\"val_x\"], valid[\"val_y\"])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_ds = CustomDataset(test[\"test_x\"], test[\"test_y\"])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size,shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "865968a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | model | MLnames | 3.5 K  | train\n",
      "------------------------------------------\n",
      "3.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68741f62e50547e8af5421e6530a97e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument #2 'mat2' is on CPU, but expected it to be on GPU (while checking arguments for mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m data_module \u001b[38;5;241m=\u001b[39m DataModule(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../names.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(lightning_model, datamodule\u001b[38;5;241m=\u001b[39mdata_module)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:329\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 329\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    332\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[101], line 21\u001b[0m, in \u001b[0;36mLightningModelExe.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     20\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 21\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, on_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, on_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[101], line 9\u001b[0m, in \u001b[0;36mLightningModelExe.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[100], line 29\u001b[0m, in \u001b[0;36mMLnames.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     28\u001b[0m   emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc[x] \u001b[38;5;66;03m# [32, 3, 2]\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m   z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(emb\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m@\u001b[39m w \u001b[38;5;241m+\u001b[39m b)\n\u001b[1;32m     30\u001b[0m   logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument #2 'mat2' is on CPU, but expected it to be on GPU (while checking arguments for mm)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = MLnames()\n",
    "    lightning_model = LightningModelExe(model, lr=0.01)\n",
    "    data_module = DataModule(data_dir=\"../names.txt\", batch_size=32)\n",
    "    trainer = L.Trainer(max_epochs=10)\n",
    "    trainer.fit(lightning_model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
